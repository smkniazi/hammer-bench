#
#   Licensed to the Apache Software Foundation (ASF) under one or more
#   contributor license agreements.  See the NOTICE file distributed with
#   this work for additional information regarding copyright ownership.
#   The ASF licenses this file to You under the Apache License, Version 2.0
#   (the "License"); you may not use this file except in compliance with
#   the License.  You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.



#|=========================================================================
#|                                                          Benchmark Types
#|                                                          ...............
#|
#|Supported Types = RAW, INTERLEAVED, and BR benchmarks
#|for RAW Bench mark set benchmark.type=RAW
#|and set the raw.* properties
#|
#|for INTERLEAVED Bench mark set 
#|benchmark.type=INTERLEAVED and set all the interleaved.* properties
#|
#|Filesystems supported are HopsFS, HDFS, CephFS, MapR-FS
#|_________________________________________________________________________
benchmark.type=RAW
benchmark.filesystem.name=HopsFS
#dont forget to change the DFS Client Parameters accordingly for different 
#filesystems


#|=========================================================================
#|                                              WarmUp Phase Configurations
#|                                              ...........................
#|increate the wait time if you are writing more files
#|_________________________________________________________________________
# "files.to.create.in.warmup.phase" is per thread value
files.to.create.in.warmup.phase=100
warmup.phase.wait.time=60000



#|=========================================================================
#|                                                  Raw bechmark properties
#|                                                  .......................
#|Long.MAX_VALUE = 922337236854775807
#|all times are in ms
#|_________________________________________________________________________
raw.create.phase.max.files.to.create=922337236854775807
#raw.mkdir.phase.duration=20000
raw.create.files.phase.duration=10000
#raw.file.append.phase.duration=20000
raw.read.files.phase.duration=10000
#raw.ls.dirs.phase.duration=20000
#raw.ls.files.phase.duration=20000
#raw.rename.files.phase.duration=60000
#raw.delete.files.phase.duration=20000
#raw.chmod.files.phase.duration=20000
#raw.chmod.dirs.phase.duration=20000
#raw.file.setReplication.phase.duration=20000
#raw.file.getInfo.phase.duration=20000
#raw.dir.getInfo.phase.duration=20000
#raw.file.change.user.phase.duration=20000
#raw.dir.change.user.phase.duration=20000



#|=========================================================================
#|                                          Interleaved bechmark properties
#|                                          ...............................
#|all parameters are integer values 
#|representing percentages of operations
#|only two decimal places is supported. all should add to 100.00
#|_________________________________________________________________________
generate.percentiles=true
#
#Spotify Workload
#
interleaved.workload.name=Spotify
interleaved.bm.duration=40000
#add block percentage added to create.files. 
interleaved.create.files.percentage=2.7
interleaved.file.append.percentage=0
interleaved.read.files.percentage=68.73
interleaved.rename.files.percentage=1.3
interleaved.ls.dirs.percentage=8.52
interleaved.ls.files.percentage=0.49
interleaved.delete.files.percentage=0.75
interleaved.chmod.files.percentage=0.02
interleaved.chmod.dirs.percentage=0.01
interleaved.mkdir.percentage=0.02
interleaved.file.setReplication.percentage=0.14
interleaved.file.getInfo.percentage=13.04
interleaved.dir.getInfo.percentage=3.96   
interleaved.file.change.user.percentage=0.0
interleaved.dir.change.user.percentage=0.32  



#|=========================================================================
#|                                       BlockReporting bechmark properties
#|                                          ...............................
#|all parameters are integer values
#|_________________________________________________________________________
br.benchmark.duration=60000
br.blocks.per.report=1000
br.blocks.per.file=10
br.files.per.dir=32
br.max.block.size=16
br.min.time.before.nextreport=0
br.max.time.before.nextreport=0
br.skip.creations=false
br.persist.database=jdbc:mysql://cloud1.sics.se:3307/hop_salman?user=hop&password=hop

#|=========================================================================
#|                                                  General file properties
#|                                                  .......................
#|file size is bytes
#|_________________________________________________________________________
replication.factor=1
append.size=0
#format list of tuples
#[(size,percentage),(size,percentage)]
#[(1024,10),(2048,90)]
#all percentages should add to 100
#file.size=[(0,100)]
file.size=[(0,0.30), (1024,6.30), (2048,7.20), (3072,8.50), (4096,9.60), (5120,10.20), (6144,10.50), (7168,10.80), (8192,11.00), (16384,11.90), (32768,13.70)]

#
#file tree depth and width parameters
#
dir.per.dir=2
files.per.dir=16

#if enable.fixed.depth.tree is set then the dir.per.dir and files.per.dir 
#parameters will be ignored and a tree of constant depth will be created
#set the depth >= 3
#
#if depth = 3 then
#  dir path =  /test/_user_UUID/hops_dir0
#  file path = /test/_user_UUID/hops_dir0/fileX
#if depth = 5 then
#  dir path =  /test/_user_UUID/added_depth_3/added_depth_4/hops_dirX
#  file path = /test/_user_UUID/added_depth_3/added_depth_4/hops_dir0/hops_file_X
enable.fixed.depth.tree=false
tree.depth=3

#|=========================================================================
#|                                                     Slave configurations
#|                                                     ....................
#|_________________________________________________________________________
num.slave.threads=1
slave.listening.port=5555

# each slave is sent warm up command after some delay. Time is in ms
master.slave.warmup.delay=500

#|=========================================================================
#|                                                          Failover Testing 
#|                                                          ................
#|_________________________________________________________________________
test.failover=false
restart.a.namenode.after=30000
failover.test.start.time=30000
failover.test.duration=240000
#slave machine that will kill the namenodes
#it should have password less public key access to the namenodes
namenode.killer=bbc1
hadoop.user=nzo


#HDFS
failover.namenodes=bbc1.sics.se,bbc2.sics.se
hadoop.sbin=/tmp/ref/apache_hadoop_distro/hadoop-2.0.4-alpha/sbin
failover.nn.restart.commands=ssh HADOOP_USER@NAMENODE killall java, ssh HADOOP_USER@NAMENODE HADOOP_SBIN/hadoop-daemon.sh start namenode,ssh HADOOP_USER@NAMENODE HADOOP_SBIN/hadoop-daemon.sh start zkfc

#HopsFS
#failover.namenodes=bbc1.sics.se,bbc2.sics.se,bbc3.sics.se,bbc4.sics.se
#hadoop.sbin=/tmp/nzo/hopsfs/sbin
#failover.nn.restart.commands=ssh HADOOP_USER@NAMENODE killall java,ssh HADOOP_USER@NAMENODE HADOOP_SBIN/hadoop-daemon.sh start namenode


#|=========================================================================
#|                                                               Test files
#|                                                               ..........
#|_________________________________________________________________________
#"base.dir" this where where the files are created in the hdfs
base.dir=/test
#this is where the results are stored on the local filesystem
results.dir=/tmp/hops-bm-master-results/



#|=========================================================================
#|                                                    Master configurations
#|                                                    .....................
#|_________________________________________________________________________
master.listening.port=4444
#list of slaves or localhost for testing
list.of.slaves=localhost



#|=========================================================================
#|                                                           Remote logging
#|                                                           .............. 
#|_________________________________________________________________________
skip.all.prompt=true
enable.remote.logging=true
remote.logging.port=6666
max.slave.failure.threshold=0

#|=========================================================================
#|                                                    DFS Client Parameters
#|                                                    ..................... 
#|_________________________________________________________________________
//values: ceph:///, maprfs:///, hdfs://mycluster, hdfs://host:port
fs.defaultFS=hdfs://bbc7:26801

#HDFS specific
dfs.ha.namenodes.mycluster=nn1,nn2
dfs.nameservices=mycluster
dfs.namenode.rpc-address.mycluster.nn1=bbc2.sics.se:11001
dfs.namenode.rpc-address.mycluster.nn2=bbc1.sics.se:11001
dfs.client.failover.proxy.provider.mycluster=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider


#HOPS-FS spcecific properties. Theses properties have no effect on HDFS 
dfs.namenode.selector-policy=ROUND_ROBIN
dsf.client.refresh.namenode.list=5000
dfs.clinet.max.retires.on.failure=5
dsf.client.initial.wait.on.retry=100

#CephFS
fs.ceph.impl=org.apache.hadoop.fs.ceph.CephFileSystem
ceph.auth.keyring=/etc/ceph/ceph.client.admin.keyring
ceph.conf.file=/etc/ceph/ceph.conf
ceph.root.dir=/
ceph.mon.address=193.10.66.136:6789
ceph.auth.id=admin



#|=========================================================================
#|                                                         Result Generator
#|                                                         ................ 
#| each result file will contain the namenode count and ndb
#| information used if multiple results are aggregated
#|_________________________________________________________________________
no.of.namenodes=2
no.of.ndb.datanodes=0

